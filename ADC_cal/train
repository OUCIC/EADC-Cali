import scipy.io as sio
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.optim.optimizer import Optimizer
import argparse
import os

import numpy as np
import sys
sys.path.append("../../")
from utils import normalize_tensor
from model import TCN_FCNN
# from op import Adahessian
import time
import matplotlib.pyplot as plt



device = torch.device('cuda' if torch.cuda.is_available() else "cpu")
print(device)


parser = argparse.ArgumentParser()
parser.add_argument('--batch_size', type=int, default=100, metavar='N',
                    help='batch size (default: 100)')
parser.add_argument('--dropout', type=float, default=0.0,
                    help='dropout applied to layers (default: 0.0)')
parser.add_argument('--clip', type=float, default=-1,
                    help='gradient clip, -1 means no clip (default: -1)')
parser.add_argument('--epochs', type=int, default=1000,
                    help='upper epoch limit (default: 1000)')
parser.add_argument('--ksize', type=int, default=8,
                    help='kernel size (default: 8)')
parser.add_argument('--levels', type=int, default=8,
                    help='# of levels (default: 8)')
parser.add_argument('--seq_len', type=int, default=400,
                    help='sequence length (default: 400)')
parser.add_argument('--log-interval', type=int, default=100, metavar='N',
                    help='report interval (default: 100')
parser.add_argument('--lr', type=float, default=0.001,
                    help='initial learning rate (default: 4e-3)')
parser.add_argument('--optim', type=str, default='Adam',
                    help='optimizer to use (default: Adam)')
parser.add_argument('--nhid', type=int, default=16,
                    help='number of hidden units per layer (default: 16)')
parser.add_argument('--seed', type=int, default=1111,
                    help='random seed (default: 1111)')
args = parser.parse_args()



input_channels = 1
output_channels = 1
batch_size = args.batch_size
epochs = args.epochs
channel_sizes = [args.nhid]*args.levels
kernel_size = args.ksize
dropout = args.dropout




data1=DataLoader('train data path')
data2=DataLoader('label data path')
tensor_data1 = torch.FloatTensor(data1)
tensor_data2 = torch.FloatTensor(data2)
X=torch.rand(tensor_data1.shape[0],tensor_data1.shape[1])
Y=torch.rand(tensor_data2.shape[0],tensor_data2.shape[1])
X=tensor_data1
Y=tensor_data2

#Normalized data
X,Y=normalize_tensor(X,Y,0,1)

dataset = TensorDataset(X, Y)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)



# Create model instances and optimizers
model = TCN_FCNN(input_channels, output_channels,channel_sizes,kernel_size=kernel_size, dropout=dropout)
model=model.to(device)
lr = args.lr
optimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)





best_loss = 1e8
for epoch in range(epochs):
    for batch_X, batch_y in dataloader:
        # batch_y = batch_y.reshape(-1, 4)  # Convert to 2D, BP neural network can only accept 2D input and output

        cali=batch_y-batch_X
        # cali=torch.reshape(cali, (-1, 1))
        # batch_y=torch.reshape(batch_y, (-1, 1))
        batch_X=batch_X.to(device)
        cali = cali.to(device)
        # batch_y=batch_y.to(device)
        # # Forward propagation
        outputs = model(batch_X.unsqueeze(1).contiguous())
        # # loss = nn.CrossEntropyLoss()(outputs, batch_y)
        loss=torch.nn.functional.mse_loss(outputs,cali)
        # Backpropagation and optimization
        optimizer.zero_grad()
        # lookahead.zero_grad()

        loss.backward()
        # lookahead.step()
        optimizer.step()
        # scheduler.step()
    if loss < best_loss:
        directory = 'model'
        if not os.path.exists(directory):
            os.makedirs(directory)
            # name = f'{input_size}_{hidden_size}_{output_size}_adam'
        print("Saved model!")

        torch.save(model.state_dict(), f'E:/your path.pt')
        best_loss = loss
    if epoch % 20 == 0:
        lr = lr * 0.9

# 打印每个epoch的损失
    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.10f}')

